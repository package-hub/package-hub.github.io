---
title: convex_adversarial
categories: ['python']
---
## [convex_adversarial](https://github.com/locuslab/convex_adversarial)

### A method for training neural networks that are provably robust to adversarial attacks. 


*A repository for training provably robust neural networks by optimizing convex outer bounds on the adversarial polytope. Created by [Eric Wong](https://riceric22.github.io) and [Zico Kolter](http://zicokolter.com). [Link to the original arXiv paper][paper]. The method has been further extended to be fully modular, scalable, and use cascades to improve robust error. Check out our new paper on arXiv: [Scaling provable adversarial defenses][scalable_paper].*

[paper]: https://arxiv.org/abs/1711.00851
[scalable_paper]: https://arxiv.org/abs/1805.12514
