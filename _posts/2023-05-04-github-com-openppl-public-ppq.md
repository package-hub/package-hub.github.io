---
title: ppq
categories: ['python', 'neural-network', 'deep-learning']
---
## [ppq](https://github.com/openppl-public/ppq)

### PPL Quantization Tool (PPQ) is a powerful offline neural network quantization tool.


PPQ 是一个可扩展的、高性能的、面向工业应用的神经网络量化工具。

神经网络量化，作为一种常用的神经网络加速方案自 2016 年以来被广泛地应用。相比于神经网络剪枝与架构搜索，网络量化的泛用性更强，具有较高的工业实用价值。特别是对于端侧芯片而言，在片上面积与功耗都受到限制的场景下，我们总是希望将所有浮点运算转换为定点运算。量化技术的价值在于浮点运算与访存是十分昂贵的，它依赖于复杂的浮点运算器以及较高的访存带宽。如果我们能够在可接受的范围内使用较低位宽的定点运算近似浮点结果，这将使得我们在芯片电路设计、系统功耗、系统延迟与吞吐量等多方面获得显著的优势。

我们正处在时代的浪潮之中，基于神经网络的人工智能正快速发展，图像识别、图像超分辨率、内容生成、模型重建等技术正改变我们的生活。与之俱来的，是不断变化的模型结构，成为摆在模型量化与部署前的第一道难关。为了处理复杂结构，我们设计了完整的计算图逻辑结构与图调度逻辑，这些努力使得 PPQ 能够解析并修改复杂的模型结构，自动判定网络中的量化区与非量化区，并允许用户对调度逻辑进行手动控制。

网络的量化与性能优化是严峻的工程问题，我们希望用户能够参与到网络的量化与部署过程中来，参与到神经网络的性能优化中来。为此我们在 Github 中提供相应的与部署相关学习资料，并在软件设计上刻意强调接口的灵活性。在我们不断的尝试与探索中，我们抽象出量化器这一逻辑类型，负责初始化不同硬件平台上的量化策略，并允许用户自定义网络中每一个算子、每一个张量的量化位宽、量化粒度与校准算法等。我们将量化逻辑重组为27个独立的量化优化过程 (Quantization Optimization Pass)，PPQ 的用户可以根据需求任意组合优化过程，完成高度灵活的量化任务。作为 PPQ 的使用者，您能够根据需求新增、修改所有优化过程，探索量化技术的新边界。

这是一个为处理复杂量化任务而生的框架 —— PPQ 的执行引擎是专为量化设计的，截止 PPQ 0.6.6 版本，软件一共内置 99 种常见的 Onnx 算子执行逻辑，并原生支持执行过程中的量化模拟操作。PPQ 可以脱离 Onnxruntime 完成 Onnx 模型的推理与量化。作为架构设计一部分，我们允许用户使用 Python + Pytorch 或 C++ / Cuda 为 PPQ 注册新的算子实现，新的逻辑亦可替换现有的算子实现逻辑。PPQ 允许相同的算子在不同平台上有不同的执行逻辑，从而支撑不同硬件平台的运行模拟。借助定制化的执行引擎与 PPQ Cuda Kernel 的高性能实现，使得 PPQ 具有极其显著的性能优势，往往能以惊人的效率完成量化任务。

PPQ 的开发与推理框架关系密切，这使得我们能够了解硬件推理的诸多细节，从而严格控制硬件模拟误差。在国内外众多开源工作者共同努力之下，目前 PPQ 支持与 TensorRT, OpenPPL, Openvino, ncnn, mnn, Onnxruntime, Tengine, Snpe, GraphCore, Metax 等多个推理框架协同工作，并预制了对应量化器与导出逻辑。PPQ 是一个高度可扩展的模型量化框架，借助 ppq.lib 中的函数功能，您能够将 PPQ 的量化能力扩展到其他可能的硬件与推理库上。我们期待与您一起把人工智慧带到千家万户之间。
